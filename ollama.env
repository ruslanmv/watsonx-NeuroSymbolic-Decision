# This is the default values to run Ollama localy. 
LLM_TYPE=LOCAL_OLLAMA
USE_NEURO_SYMBOLIC=0
OLLAMA_SERVER_URL=http://host.docker.internal:11434
# Uncomment to use the IBM Granite model.
# OLLAMA_MODEL_NAME=granite3-dense:8b
OLLAMA_MODEL_NAME=mistral

